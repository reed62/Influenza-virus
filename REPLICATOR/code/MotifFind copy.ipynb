{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from itertools import product\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import pearsonr\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "from src.bio_utils import seqlogo_from_msa\n",
    "import logomaker as lm\n",
    "from scipy.stats import mode\n",
    "from math import ceil, floor, sqrt\n",
    "from sklearn.metrics import r2_score\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "import RNA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def add_transcrib_eff(virus_name, date):\n",
    "    data_df = pd.read_csv(f\"./data/{virus_name}_{date}.csv\")\n",
    "    utr_df = pd.read_csv('../data/sequencing/tol_seq.csv', usecols=['utr','pre_exp'])\n",
    "    data_df['utr'] = data_df.seq.str[:5]\n",
    "    merge_df = pd.merge(left=data_df,right=utr_df,how='inner',on='utr')\n",
    "    merge_df['score_final'] = merge_df.score - merge_df.pre_exp\n",
    "    merge_df.to_csv(f'./data/{virus_name}_{date}_tol_seq.csv',index=None)\n",
    "\n",
    "def seq2tensor(X):\n",
    "    corpus = [\"\".join(s) for s in list(product(*[\"ACGU\"]))]\n",
    "    tok2idx = {s: i for i, s in enumerate(corpus)}\n",
    "    X = [[tok2idx[X[j][i : i + 1]] for i in range(len(X[j]))] for j in range(len(X))]\n",
    "    X = torch.tensor(X).type(torch.int64)\n",
    "    X = F.one_hot(X,num_classes=4).type(torch.float)\n",
    "    return X\n",
    "\n",
    "def struc2tensor(X):\n",
    "    corpus = [\"\".join(s) for s in list(product(*[\".()\"]))]\n",
    "    tok2idx = {s: i for i, s in enumerate(corpus)}\n",
    "    X = [[tok2idx[X[j][i : i + 1]] for i in range(len(X[j]))] for j in range(len(X))]\n",
    "    X = torch.tensor(X).type(torch.int64)\n",
    "    X = F.one_hot(X,num_classes=3).type(torch.float)\n",
    "    return X\n",
    "\n",
    "def seq2tsor_transformer(X):\n",
    "    corpus = [\"\".join(s) for s in list(product(*[\"ACGU\"]))]\n",
    "    tok2idx = {s: i for i, s in enumerate(corpus)}\n",
    "    X = [[tok2idx[X[j][i : i + 1]] for i in range(len(X[j]))] for j in range(len(X))]\n",
    "    X = torch.LongTensor(X)\n",
    "    return X\n",
    "\n",
    "def random_split(virus_name,date):\n",
    "    merge_df = pd.read_csv(f'./data/{virus_name}_{date}_tol_seq.csv')\n",
    "    train, test = train_test_split(merge_df, test_size=0.05)\n",
    "    train.to_csv(f'./data/{virus_name}_{date}_train_tol_seq.csv',index=None)\n",
    "    test.to_csv(f'./data/{virus_name}_{date}_test_tol_seq.csv',index=None)\n",
    "\n",
    "def split_for_distribu(virus_name,date):\n",
    "    data1 = pd.read_csv(f'./data/{virus_name}_{date}_tol_seq.csv')\n",
    "    data1 = data1.sort_values(by = [ 'distance' , 'rna_counts' ], ascending= ( True , False ))\n",
    "    test_data = pd.DataFrame(columns=data1.columns)\n",
    "    train_data = pd.DataFrame(columns=data1.columns)\n",
    "    for i in range(1 , data1.distance.max()+1 , 1):\n",
    "        data2 = data1.loc[data1['distance'] == i]\n",
    "        for j in range(int(len(data2) / 20 )):\n",
    "            df1 = data2.iloc[1 + j * 19]  \n",
    "            test_data = pd.concat([test_data,pd.DataFrame(df1).transpose()])\n",
    "    df2 = pd.concat([data1,test_data])\n",
    "    train_data = df2.drop_duplicates(subset=['ID'],keep=False)\n",
    "    train_data = train_data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    train_data.to_csv(f'./data/{virus_name}_{date}_train_tol_seq.csv', index = None)\n",
    "    test_data.to_csv(f'./data/{virus_name}_{date}_test_tol_seq.csv', index = None)\n",
    "\n",
    "def count_paras(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# VEE\n",
    "para_list = []\n",
    "for lay1 in range(3,10,2):\n",
    "    for lay2 in range(3,19,4):\n",
    "        for lay3 in range(3,13,2):\n",
    "            para_list.append([lay1,lay2,lay3])\n",
    "len(para_list)\n",
    "# para_list\n",
    "# SFV\n",
    "para_list = []\n",
    "for lay1 in range(3,23,4):\n",
    "    for lay2 in range(3,23,4):\n",
    "        for lay3 in range(3,23,4):\n",
    "            para_list.append([lay1,lay2,lay3])\n",
    "len(para_list)\n",
    "# para_list\n",
    "\n",
    "para_list = []\n",
    "for lay1 in range(0,85-11):\n",
    "        para_list.append([lay1,lay1+11])\n",
    "len(para_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MotifFind(nn.Module):\n",
    "    def __init__(self,paras):\n",
    "        super(MotifFind, self).__init__()\n",
    "        self.convs1_l1 = nn.Conv1d(4, 1, paras[0], padding='same')\n",
    "        self.convs1_l2 = nn.Conv1d(4, 1, paras[1], padding='same') \n",
    "        self.convs1_l3 = nn.Conv1d(4, 1, paras[2], padding='same')\n",
    "        self.convs2_l = nn.Conv1d(3*1, paras[3], 21, padding='same')   \n",
    "        self.actia = nn.Softplus()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = x[:, :, 0:10]  # 20\n",
    "        x2 = x[:, :, 11:30] # 21:40\n",
    "        x3 = x[:, :,31:44] # 41:60\n",
    "        # x1 = x[:, :, 0:20]  # 20\n",
    "        # x2 = x[:, :, 21:40] # 21:40\n",
    "        # x3 = x[:, :, 41:60] # 41:60\n",
    "        c1_l1 = self.actia(self.convs1_l1(x1))\n",
    "        c1_l2 = self.actia(self.convs1_l2(x2))\n",
    "        c1_l3 = self.actia(self.convs1_l3(x3))\n",
    "        max_len = 21\n",
    "        c1_l1 = F.pad(c1_l1, (0, max_len - c1_l1.size(2)))\n",
    "        c1_l2 = F.pad(c1_l2, (0, max_len - c1_l2.size(2)))\n",
    "        c1_l3 = F.pad(c1_l3, (0, max_len - c1_l3.size(2)))\n",
    "        c1_l = torch.cat((c1_l1, c1_l2, c1_l3), dim=1)    \n",
    "        c2_l = self.actia(self.convs2_l(c1_l))\n",
    "        cnn_out1 = F.max_pool1d(c2_l, int(c2_l.size(2)))\n",
    "        cnn_out = cnn_out1.squeeze(2)\n",
    "        y_pre = torch.log(cnn_out.sum(axis=1))\n",
    "        return y_pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [01:20<00:00,  1.62s/it]\n"
     ]
    }
   ],
   "source": [
    "num = 0\n",
    "virus_name=\"SFV\"\n",
    "# for paras in para_list: # 15,9,9\n",
    "for paras in [[15,15,9,9]]:\n",
    "    num +=1\n",
    "    time_start = time.time()\n",
    "    model = MotifFind(paras=paras)\n",
    "    model_name = 'Motifind'\n",
    "\n",
    "    data = pd.read_csv(\"./data/SFV_train.csv\", \n",
    "                    usecols=['seq','score'])\n",
    "    data.columns = ['seq','score']\n",
    "    model.train()\n",
    "    criterion = nn.MSELoss()\n",
    "    batch_num = 32\n",
    "    epochs = 50\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=5e-3) # suitable for motiffind deepcnn\n",
    "    epoch_loss = []\n",
    "    epoch_r = []\n",
    "    epoch_r2 = []\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        losses = []\n",
    "        pearsonrs = []\n",
    "        r2s = []\n",
    "        data_random = data.sample(n=len(data)).reset_index(drop=True)\n",
    "        seq = data_random.seq.tolist()\n",
    "        score = data_random.score.tolist()\n",
    "        for i in range(0,len(seq),batch_num):\n",
    "            X = seq[i:i+batch_num]\n",
    "            Y = score[i:i+batch_num]\n",
    "            Y = torch.tensor(Y).type(torch.float)\n",
    "            X = seq2tensor(X).permute(0,2,1)\n",
    "            output = model(X)\n",
    "            loss = criterion(output, Y)\n",
    "            # regularization\n",
    "            # l_reg = torch.norm(model.convs1_l.weight)**2\n",
    "            # loss += model.alpha * l_reg\n",
    "            model.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            losses.append(loss.item())\n",
    "            r,_ = pearsonr(output.detach().numpy(), Y.detach().numpy())\n",
    "            r2 = r2_score(Y.detach().numpy(), output.detach().numpy())\n",
    "            pearsonrs.append(r)\n",
    "            r2s.append(r2)\n",
    "        epoch_loss.append(np.mean(losses))\n",
    "        epoch_r.append(np.mean(pearsonrs))\n",
    "        epoch_r2.append(np.mean(r2s))\n",
    "    time_end = time.time()\n",
    "    time_cost = time_end - time_start\n",
    "\n",
    "\n",
    "    model.eval()\n",
    "    test_data = pd.read_csv(\"./data/SFV_test.csv\", \n",
    "                    usecols=['seq','score'])\n",
    "    test_data.columns = ['seq','score']\n",
    "    test_seq = test_data.seq.tolist()\n",
    "    test_score = test_data.score.tolist()\n",
    "    X = test_seq\n",
    "    Y = test_score\n",
    "    Y = torch.tensor(Y).type(torch.float)\n",
    "    X = seq2tensor(X).permute(0,2,1)\n",
    "    criterion = nn.MSELoss()\n",
    "    with torch.no_grad():\n",
    "        output = model(X)\n",
    "        loss = criterion(output, Y)\n",
    "\n",
    "    test_r, p_val = pearsonr(output.detach().numpy(), Y.detach().numpy())\n",
    "    test_r2 = r2_score(Y.detach().numpy(), output.detach().numpy())\n",
    "\n",
    "    \"\"\"np.save(f\"./model/{model_name}_{virus_name}_paras_{paras[3]}.npy\", {\"epoch_loss\": epoch_loss, \"epoch_r\": epoch_r, 'time_cost': time_cost, 'paras':paras, \n",
    "            'test_r':test_r,'test_r2':test_r2,}\n",
    "            )\"\"\"\n",
    "    torch.save(model.state_dict(), f\"./model/{model_name}_{virus_name}_paras_{paras[3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [01:45<00:00,  2.11s/it]\n"
     ]
    }
   ],
   "source": [
    "num = 0\n",
    "virus_name=\"VEE\"\n",
    "# for paras in para_list: # 15,9,9\n",
    "for paras in [[15,15,15,3]]:\n",
    "    num +=1\n",
    "    time_start = time.time()\n",
    "    model = MotifFind(paras=paras)\n",
    "    model_name = 'Motifind'\n",
    "    # model = DeepCNN_BiLSTM()\n",
    "    # model_name = 'DeepCNN_BiLSTM'\n",
    "\n",
    "    data = pd.read_csv(\"./data/VEE_train.csv\",\n",
    "                    usecols=['seq','score'])\n",
    "    data.columns = ['seq','score']\n",
    "    model.train()\n",
    "    criterion = nn.MSELoss()\n",
    "    batch_num = 32\n",
    "    epochs = 50\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=5e-3) # suitable for motiffind deepcnn\n",
    "    epoch_loss = []\n",
    "    epoch_r = []\n",
    "    epoch_r2 = []\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        losses = []\n",
    "        pearsonrs = []\n",
    "        r2s = []\n",
    "        data_random = data.sample(n=len(data)).reset_index(drop=True)\n",
    "        seq = data_random.seq.tolist()\n",
    "        score = data_random.score.tolist()\n",
    "        for i in range(0,len(seq),batch_num):\n",
    "            X = seq[i:i+batch_num]\n",
    "            Y = score[i:i+batch_num]\n",
    "            Y = torch.tensor(Y).type(torch.float)\n",
    "            X = seq2tensor(X).permute(0,2,1)\n",
    "            output = model(X)\n",
    "            loss = criterion(output, Y)\n",
    "            # regularization\n",
    "            # l_reg = torch.norm(model.convs1_l.weight)**2\n",
    "            # loss += model.alpha * l_reg\n",
    "            model.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            losses.append(loss.item())\n",
    "            r,_ = pearsonr(output.detach().numpy(), Y.detach().numpy())\n",
    "            r2 = r2_score(Y.detach().numpy(), output.detach().numpy())\n",
    "            pearsonrs.append(r)\n",
    "            r2s.append(r2)\n",
    "        epoch_loss.append(np.mean(losses))\n",
    "        epoch_r.append(np.mean(pearsonrs))\n",
    "        epoch_r2.append(np.mean(r2s))\n",
    "    time_end = time.time()\n",
    "    time_cost = time_end - time_start\n",
    "\n",
    "\n",
    "    model.eval()\n",
    "    test_data = pd.read_csv(\"./data/VEE_test.csv\",\n",
    "                    usecols=['seq','score'])\n",
    "    test_data.columns = ['seq','score']\n",
    "    test_seq = test_data.seq.tolist()\n",
    "    test_score = test_data.score.tolist()\n",
    "    X = test_seq\n",
    "    Y = test_score\n",
    "    Y = torch.tensor(Y).type(torch.float)\n",
    "    X = seq2tensor(X).permute(0,2,1)\n",
    "    criterion = nn.MSELoss()\n",
    "    with torch.no_grad():\n",
    "        output = model(X)\n",
    "        loss = criterion(output, Y)\n",
    "\n",
    "    test_r, p_val = pearsonr(output.detach().numpy(), Y.detach().numpy())\n",
    "    test_r2 = r2_score(Y.detach().numpy(), output.detach().numpy())\n",
    "\n",
    "    \"\"\"np.save(f\"./model/{model_name}_{virus_name}_paras_{paras[3]}.npy\", {\"epoch_loss\": epoch_loss, \"epoch_r\": epoch_r, 'time_cost': time_cost, 'paras':paras, \n",
    "            'test_r':test_r,'test_r2':test_r2,}\n",
    "            )\"\"\"\n",
    "    torch.save(model.state_dict(), f\"./model/{model_name}_{virus_name}_paras_{paras[3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MotifFind(\n",
       "  (convs1_l1): Conv1d(4, 1, kernel_size=(15,), stride=(1,), padding=same)\n",
       "  (convs1_l2): Conv1d(4, 1, kernel_size=(15,), stride=(1,), padding=same)\n",
       "  (convs1_l3): Conv1d(4, 1, kernel_size=(9,), stride=(1,), padding=same)\n",
       "  (convs2_l): Conv1d(3, 9, kernel_size=(21,), stride=(1,), padding=same)\n",
       "  (actia): Softplus(beta=1.0, threshold=20.0)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MotifFind(paras=[15,15,9,9])\n",
    "model_name = 'MotifFind'\n",
    "model.load_state_dict(torch.load(\"./model/Motifind_SFV_paras_9\"))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MotifFind(\n",
       "  (convs1_l1): Conv1d(4, 1, kernel_size=(15,), stride=(1,), padding=same)\n",
       "  (convs1_l2): Conv1d(4, 1, kernel_size=(15,), stride=(1,), padding=same)\n",
       "  (convs1_l3): Conv1d(4, 1, kernel_size=(15,), stride=(1,), padding=same)\n",
       "  (convs2_l): Conv1d(3, 3, kernel_size=(21,), stride=(1,), padding=same)\n",
       "  (actia): Softplus(beta=1.0, threshold=20.0)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MotifFind(paras=[15,15,15,3])\n",
    "model_name = 'MotifFind'\n",
    "model.load_state_dict(torch.load(\"./model/Motifind_VEE_paras_3\"))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "test_seq = test_data.seq.tolist()\n",
    "test_score = test_data.score.tolist()\n",
    "\n",
    "X = seq\n",
    "Y = score\n",
    "Y = torch.tensor(Y).type(torch.float)\n",
    "X = seq2tensor(X).permute(0,2,1)\n",
    "criterion = nn.MSELoss()\n",
    "with torch.no_grad():\n",
    "    output = model(X)\n",
    "    loss = criterion(output, Y)\n",
    "\n",
    "_, ax = plt.subplots(1, 1, figsize=(4,4))\n",
    "ax.scatter( Y.detach().numpy(), output.detach().numpy(),alpha=0.45, s=50, facecolor=\"gray\")\n",
    "r, p_val = pearsonr(output.detach().numpy(), Y.detach().numpy())\n",
    "print(r, p_val)\n",
    "r2 = r2_score(Y.detach().numpy(), output.detach().numpy())\n",
    "print(r2)\n",
    "\n",
    "X = test_seq\n",
    "Y = test_score\n",
    "Y = torch.tensor(Y).type(torch.float)\n",
    "X = seq2tensor(X).permute(0,2,1)\n",
    "criterion = nn.MSELoss()\n",
    "with torch.no_grad():\n",
    "    output = model(X)\n",
    "    loss = criterion(output, Y)\n",
    "\n",
    "_, ax = plt.subplots(1, 1, figsize=(4,4))\n",
    "ax.scatter( Y.detach().numpy(), output.detach().numpy(),alpha=0.45, s=50, facecolor=\"gray\")\n",
    "r, p_val = pearsonr(output.detach().numpy(), Y.detach().numpy())\n",
    "print(r, p_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "X = seq\n",
    "Y = score\n",
    "Y = torch.tensor(Y).type(torch.float)\n",
    "X = seq2tensor(X).permute(0,2,1)\n",
    "x1 = X[:, :, 0:10]  # 20\n",
    "x2 = X[:, :, 20:40] # 21:40\n",
    "x3 = X[:, :, 41:60] # 41:60\n",
    "# x1 = X[:, :, 0:20]  # 20\n",
    "# x2 = X[:, :, 21:40] # 21:40\n",
    "# x3 = X[:, :, 41:60] # 41:60\n",
    "layer_output = model.actia(model.convs1_l1(x1)).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_size =7\n",
    "window_idx = 0\n",
    "window = 'l1'\n",
    "if virus_name==\"SFV\":\n",
    "    df = pd.read_csv(\"../data/alphavirus/SFV_RandomMutants_Final.csv\")\n",
    "else:\n",
    "    df = pd.read_csv(\"../data/alphavirus/VEE_RandomMutants_Final.csv\")\n",
    "def generate_positional_counts_mat(seqs, centers, filter_size):\n",
    "    counts_df_group = []\n",
    "    center_dict = dict(Counter(centers))\n",
    "    for i in range(filter_size):\n",
    "        nt_counts_all = {\"A\": 0, \"U\": 0, \"C\": 0, \"G\": 0}\n",
    "        for center in center_dict.keys():\n",
    "            if (\n",
    "                center - filter_size // 2 + i >= 0\n",
    "                and center - filter_size // 2 + i < len(seqs[0])\n",
    "            ):\n",
    "                nt_counts = dict(\n",
    "                    Counter([s[center - filter_size // 2 + i] for s in seqs])\n",
    "                )\n",
    "            else:\n",
    "                nt_counts = {\"A\": 1, \"U\": 1, \"C\": 1, \"G\": 1}\n",
    "            for nt in nt_counts.keys():\n",
    "                nt_counts_all[nt] += nt_counts[nt] * center_dict[center]\n",
    "        counts_df_group.append(pd.DataFrame.from_records([nt_counts_all]))\n",
    "    counts_df = pd.concat(counts_df_group).reset_index(drop=True)\n",
    "    return counts_df\n",
    "\n",
    "bg_seqs = df[\"seq\"].tolist()\n",
    "kernel_data = []\n",
    "kernel_data_dict = []\n",
    "for i in range(layer_output.shape[1]):\n",
    "# for i in range(0,1):\n",
    "    curr_kernel_output = layer_output[:, i, :]\n",
    "    \n",
    "    max_activ_idx = np.array(\n",
    "        [\n",
    "            np.argmax(curr_kernel_output[j, :]) + window_idx\n",
    "            for j in range(curr_kernel_output.shape[0])\n",
    "        ]\n",
    "    )\n",
    "    max_activation_vals = np.max(curr_kernel_output, axis=1)\n",
    "    activ_thresh = max_activation_vals[\n",
    "        np.argsort(max_activation_vals)[-len(max_activation_vals) // 10]\n",
    "    ]\n",
    "    activ_indices = np.where(max_activation_vals > activ_thresh)[0]\n",
    "\n",
    "    # Abort if too few sequences were activated.\n",
    "    if len(activ_indices) < 5:\n",
    "        continue\n",
    "\n",
    "    seqs = seq\n",
    "\n",
    "    motifs = []\n",
    "    centers = []\n",
    "    for j, center in enumerate(max_activ_idx):\n",
    "        if (\n",
    "            center >= filter_size // 2\n",
    "            and center <= len(seqs[0]) - filter_size // 2 - 1\n",
    "        ):\n",
    "            motif = seqs[j][\n",
    "                center - filter_size // 2 : center + filter_size // 2 + 1\n",
    "            ]\n",
    "        elif center < filter_size // 2:\n",
    "            motif = seqs[j][0 : center + filter_size // 2 + 1]\n",
    "            motif = \"-\" * (filter_size - len(motif)) + motif\n",
    "        elif center > len(seqs[0]) - filter_size // 2 - 1:\n",
    "            motif = seqs[j][center - filter_size // 2 :]\n",
    "            motif += \"-\" * (filter_size - len(motif))\n",
    "        motifs.append(motif)\n",
    "        centers.append(center)\n",
    "\n",
    "    bg_counts_df = generate_positional_counts_mat(\n",
    "        bg_seqs, centers, filter_size\n",
    "    )\n",
    "    counts_df = seqlogo_from_msa(motifs, bg_counts_mat=bg_counts_df)  \n",
    "    # counts_df = lm.alignment_to_matrix(motifs, to_type='information',background=bg_counts_df)\n",
    "    kernel_scores = np.array(\n",
    "        [\n",
    "            pearsonr(layer_output[:, i, loc], Y.detach().numpy())[0]\n",
    "            for loc in range(layer_output.shape[2])\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    kernel_data.append(\n",
    "                (counts_df, i, mode(centers)[0], len(motifs), kernel_scores)\n",
    "            )\n",
    "\n",
    "    order_counts_df = bg_counts_df[['A', 'C', 'G', 'U']]\n",
    "    kernel_data_dict.append(\n",
    "        (order_counts_df, i, mode(centers)[0], len(motifs), kernel_scores)\n",
    "    )\n",
    "\n",
    "kernel_data.sort(key=lambda elem: elem[2])\n",
    "kernel_data_dict.sort(key=lambda elem: elem[2])\n",
    "\n",
    "n = len(kernel_data)\n",
    "# Set figure\n",
    "col = floor(sqrt(n))\n",
    "row = ceil(n / col)\n",
    "\n",
    "fig = plt.figure(figsize=(col * 10, row * 5))\n",
    "gs = plt.GridSpec(row * 2, col, figure=fig, height_ratios=[10, 1] * row)\n",
    "\n",
    "for i, (counts_df, idx, kernel_loc, n_motifs, kernel_scores) in enumerate(\n",
    "    kernel_data[:n]\n",
    "):\n",
    "    _, ax = plt.subplots(1, 1, figsize=(filter_size/2,3))\n",
    "    lm.Logo(counts_df, ax=ax, color_scheme=\"classic\",font_name='No')\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['bottom'].set_linewidth(2)\n",
    "    ax.spines['left'].set_linewidth(2)\n",
    "    ax.set_xticks([])\n",
    "    ax.tick_params(width=2) \n",
    "    #ax.set_yticks([0,0.1,0.15])\n",
    "    # ax.text(\n",
    "    #     len(counts_df) // 2,\n",
    "    #     3.96,\n",
    "    #     \"Kernel %d: center=%d, n=%d\" % (idx, kernel_loc, n_motifs),\n",
    "    #     ha=\"center\",\n",
    "    #     va=\"top\",\n",
    "    #     fontsize=12,\n",
    "    # )\n",
    "    # ax.set_yticks([])\n",
    "    # ax2 = fig.add_subplot(gs[i // col * 2 + 1, i % col])\n",
    "    # sns.heatmap(\n",
    "    #     kernel_scores.reshape((1, -1)),\n",
    "    #     ax=ax2,\n",
    "    #     cmap=\"seismic\",\n",
    "    #     vmin=-1,\n",
    "    #     vmax=1,\n",
    "    #     cbar=False,\n",
    "    # )\n",
    "    # ax2.axis(\"off\")\n",
    "# save_fig(\n",
    "#     f\"{virus_name}_{date}_{model_name}_conv1_{window}_withE\"\n",
    "# )\n",
    "plt.rcParams['svg.fonttype'] = 'none'\n",
    "plt.savefig(\"./results/SFV_a.eps\", format=\"eps\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
