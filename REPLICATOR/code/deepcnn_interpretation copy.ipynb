{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('./src')\n",
    "import torch\n",
    "from src.model_config import ModelConfig, model_collections\n",
    "from src.bio_utils import seqlogo_from_msa\n",
    "import data\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from pathlib import Path\n",
    "import seaborn as sns\n",
    "import logomaker as lm\n",
    "from scipy.stats import pearsonr, mode\n",
    "from math import sqrt, floor, ceil\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "import sys\n",
    "import torch\n",
    "from src.data import NormalDataset, BERTDataset,LRDataset\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "from scipy.stats import pearsonr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepCNN_biLSTM(\n",
      "  (convs): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Conv1d(4, 128, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "      (1): ReLU()\n",
      "      (2): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): Conv1d(128, 128, kernel_size=(9,), stride=(1,), padding=(4,))\n",
      "      (1): ReLU()\n",
      "      (2): Dropout(p=0.2, inplace=False)\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): Conv1d(128, 128, kernel_size=(11,), stride=(1,), padding=(5,))\n",
      "      (1): ReLU()\n",
      "      (2): Dropout(p=0.2, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (lstm): LSTM(128, 128, num_layers=3, bidirectional=True)\n",
      "  (fc): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear(in_features=256, out_features=64, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Dropout(p=0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (reg): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Define the model\n",
    "model_name=\"deepcnnlstm_in\"\n",
    "mconf = ModelConfig(model_name)\n",
    "model = model_collections[model_name](mconf)\n",
    "\n",
    "activation = {}\n",
    "\n",
    "# Hook function\n",
    "def get_activation(name):\n",
    "    def hook(model, input, output):\n",
    "        activation[name] = output.detach()\n",
    "    return hook\n",
    "\n",
    "# Load data \n",
    "model.load_state_dict(torch.load(Path(\"./model/in.0410.deepcnnlstm.params.train.13547_5\")))\n",
    "model.eval()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepCNN_biLSTM(\n",
      "  (convs): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Conv1d(4, 128, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "      (1): ReLU()\n",
      "      (2): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): Conv1d(128, 128, kernel_size=(9,), stride=(1,), padding=(4,))\n",
      "      (1): ReLU()\n",
      "      (2): Dropout(p=0.2, inplace=False)\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): Conv1d(128, 128, kernel_size=(11,), stride=(1,), padding=(5,))\n",
      "      (1): ReLU()\n",
      "      (2): Dropout(p=0.2, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (lstm): LSTM(128, 128, num_layers=3, bidirectional=True)\n",
      "  (fc): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear(in_features=256, out_features=64, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Dropout(p=0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (reg): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Define the model\n",
    "model_name=\"deepcnnlstm_in\"\n",
    "mconf = ModelConfig(model_name)\n",
    "model = model_collections[model_name](mconf)\n",
    "\n",
    "activation = {}\n",
    "\n",
    "# Hook function\n",
    "def get_activation(name):\n",
    "    def hook(model, input, output):\n",
    "        activation[name] = output.detach()\n",
    "    return hook\n",
    "\n",
    "# Load data \n",
    "model.load_state_dict(torch.load(Path(\"./model/12.0617.deepcnnlstm_in.params.train.5932_5\")))\n",
    "model.eval()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "virus_name = \"in\"\n",
    "model_name = \"deepcnnlstm_in\"\n",
    "test_path =\"./data/wt_filter_gate_test.csv\"\n",
    "data_size = len(pd.read_csv(test_path))\n",
    "if model_name == \"bert\":\n",
    "    test_data = BERTDataset(test_path, 26,pretrain=False)\n",
    "elif model_name==\"lr\":\n",
    "    test_data = LRDataset(test_path, kmer=3)\n",
    "else:\n",
    "    test_data = NormalDataset(test_path)\n",
    "\n",
    "loader = DataLoader(test_data, batch_size=data_size)\n",
    "\n",
    "\n",
    "# Define the model\n",
    "mconf = ModelConfig(model_name)\n",
    "model = model_collections[model_name](mconf)\n",
    "\n",
    "model.load_state_dict(torch.load(\"./model/in.0410.deepcnnlstm.params.train.13547_5\"),strict=False)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in loader:\n",
    "        out, loss = model(*batch)\n",
    "\n",
    "out = out.cpu().numpy()\n",
    "y_true = batch[1].cpu().numpy()\n",
    "print(\"Number of test data: %d\"%(len(out)))\n",
    "r, _ = pearsonr(out, y_true)\n",
    "print(r)\n",
    "\n",
    "_, ax = plt.subplots(1, 1, figsize=(4, 4))\n",
    "ax.scatter(y_true+1, out+1, alpha=0.3, color='gray', s=50)\n",
    "ax.plot([min(y_true), max(y_true)+1], [min(y_true), max(y_true)+1], linewidth=2, ls='--', color='k')\n",
    "ax.text(min(y_true), max(y_true), \"Pearson $r^2=%.3f$\"%r**2, fontsize=12)\n",
    "ax.text(min(y_true), max(y_true)-1, \"Pearson r=%.3f\" %r, fontsize=12)\n",
    "ax.text(min(y_true), max(y_true)-2, \"n=%d\" %len(out), fontsize=12)\n",
    "ax.set_xlabel('Measured score')\n",
    "ax.set_ylabel('Predicted score')\n",
    "plt.rcParams['svg.fonttype']= 'none'\n",
    "plt.savefig(\"./results/test.svg\", format='svg', bbox_inches='tight')\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "plt.gca().spines['top'].set_visible(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motifs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract all motifs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import test set\n",
    "import pandas as pd\n",
    "test_path =\"./data/wt_filter_gate_test.csv\"\n",
    "data_size = len(pd.read_csv(test_path))\n",
    "test_data = data.NormalDataset(test_path)\n",
    "loader = DataLoader(test_data, batch_size=data_size)\n",
    "filter_sizes = ModelConfig(model_name).conv_filter_sizes\n",
    "retrieve_layer = \"conv1\" \n",
    "if retrieve_layer == \"conv1\":\n",
    "    model.convs[0][0].register_forward_hook(get_activation(\"conv1\"))\n",
    "    filter_size = filter_sizes[0]\n",
    "elif retrieve_layer == \"conv2\":\n",
    "    model.convs[1][0].register_forward_hook(get_activation(\"conv2\"))\n",
    "    filter_size = filter_sizes[1] # + filter_sizes[0] // 2\n",
    "elif retrieve_layer == \"conv3\":\n",
    "    model.convs[2][0].register_forward_hook(get_activation(\"conv3\"))\n",
    "    filter_size = filter_sizes[2] # + filter_sizes[1] // 2\n",
    "    # filter_size = 13\n",
    "\n",
    "# Forward the test set\n",
    "with torch.no_grad():\n",
    "    for batch in loader:\n",
    "        out, loss = model(*batch)\n",
    "\n",
    "first_layer_output = activation[retrieve_layer].data.numpy()\n",
    "\n",
    "# Find maximal activation\n",
    "max_activation_val = first_layer_output.max()\n",
    "\n",
    "\n",
    "# Input data\n",
    "X = batch[0].cpu().numpy()\n",
    "y = batch[1].cpu().numpy()\n",
    "kernel_scores = np.array([\n",
    "    pearsonr(np.max(first_layer_output[:, i, :], axis=1), y)[0]\n",
    "     for i in range(first_layer_output.shape[1])\n",
    "])\n",
    "\n",
    "# Traverse every kernel\n",
    "kernel_data_dict = []\n",
    "\n",
    "df = pd.read_csv(\"../data/alphavirus/INFL_RandomMutants_Final.csv\")\n",
    "bg_seqs = df[\"seq\"].tolist()\n",
    "\n",
    "def generate_positional_counts_mat(seqs, centers, filter_size):\n",
    "    seqs = bg_seqs\n",
    "    counts_df_group = []\n",
    "    center_dict = dict(Counter(centers))\n",
    "    for i in range(filter_size):\n",
    "        nt_counts_all = {'A': 0, 'U': 0, 'C': 0, 'G': 0}\n",
    "        for center in center_dict.keys():\n",
    "            nt_counts = dict(Counter([s[center - filter_size // 2 + i] for s in seqs]))\n",
    "            for nt in nt_counts_all.keys():\n",
    "                nt_counts_all[nt] += nt_counts[nt] * center_dict[center]\n",
    "        counts_df_group.append(pd.DataFrame.from_records([nt_counts_all]))\n",
    "    counts_df = pd.concat(counts_df_group).reset_index(drop=True)\n",
    "    return counts_df\n",
    "\n",
    "for i in tqdm(range(first_layer_output.shape[1])):\n",
    "    kernel_output = first_layer_output[:, i, :]\n",
    "    max_val_locations = np.array(\n",
    "        [np.argmax(kernel_output[j, :]) for j in range(kernel_output.shape[0])]\n",
    "    )\n",
    "    max_activation_vals = np.max(kernel_output, axis=1)\n",
    "    # print(max_activation_vals)\n",
    "\n",
    "    # Only focus on the sequences which result in activation of 0.7 * maximum\n",
    "    indices = np.where(max_activation_vals > 0.2 * max_activation_val)[0]\n",
    "\n",
    "    # Convert embeddings back to sequences\n",
    "    seqs = [''.join([test_data.corpus[i] for i in X[idx, :]]) for idx in range(X.shape[0])]\n",
    "    motifs = []\n",
    "    centers = []\n",
    "    for j, center in enumerate(max_val_locations[indices]):\n",
    "        if center >= filter_size // 2 and center < len(seqs[0]) - filter_size // 2:\n",
    "            motif = seqs[j][center - filter_size // 2 : center + filter_size // 2 + 1]\n",
    "            if len(motif) == filter_size:  # Ensure the motif length is correct\n",
    "                motifs.append(motif)\n",
    "                centers.append(center)\n",
    "\n",
    "\n",
    "    if len(motifs) < 50:\n",
    "        counts_df = pd.DataFrame(0, index=range(filter_size), columns=[\"A\", \"U\", \"C\", \"G\"])\n",
    "        kernel_data_dict.append((counts_df, i, 0, 0, 0)) \n",
    "    else:\n",
    "        bg_counts_df = generate_positional_counts_mat(bg_seqs, centers, filter_size)\n",
    "        # bg_counts_df = bg_counts_df[['A', 'C', 'G', 'T']]\n",
    "        \n",
    "        # print(centers)\n",
    "        counts_df = seqlogo_from_msa(motifs, bg_counts_mat=bg_counts_df) \n",
    "        kernel_data_dict.append((counts_df, i, kernel_scores[i], mode(centers)[0], len(motifs)))\n",
    "        # kernel_data_dict.append((bg_counts_df, i, kernel_scores[i], mode(centers)[0], len(motifs)))\n",
    "        \n",
    "        # print(mode(centers)[0])\n",
    "\n",
    "# sort motif on location \n",
    "def takeSecond(elem):\n",
    "    return elem[1]\n",
    "kernel_data_dict.sort(key=takeSecond)\n",
    "print(kernel_data_dict)\n",
    "print(len(kernel_data_dict))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_data = []\n",
    "\n",
    "for counts_df, kernel_id, score, mode_center, motif_count in kernel_data_dict:\n",
    "    for position in range(len(counts_df)):\n",
    "        row = counts_df.iloc[position]\n",
    "        flat_data.append({\n",
    "            \"kernel_id\": kernel_id,\n",
    "            \"position\": position,\n",
    "            \"score\": score,\n",
    "            \"mode_center\": mode_center,\n",
    "            \"motif_count\": motif_count,\n",
    "            \"A\": row[\"A\"],\n",
    "            \"U\": row[\"U\"],\n",
    "            \"C\": row[\"C\"],\n",
    "            \"G\": row[\"G\"]\n",
    "        })\n",
    "\n",
    "\n",
    "kernel_df = pd.DataFrame(flat_data)\n",
    "kernel_df.to_csv(\"./results/kernel_motifs_WT.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import test set\n",
    "import pandas as pd\n",
    "test_path =\"./data/12_filter_gate_test.csv\"\n",
    "data_size = len(pd.read_csv(test_path))\n",
    "test_data = data.NormalDataset(test_path)\n",
    "loader = DataLoader(test_data, batch_size=data_size)\n",
    "filter_sizes = ModelConfig(model_name).conv_filter_sizes\n",
    "retrieve_layer = \"conv1\" \n",
    "if retrieve_layer == \"conv1\":\n",
    "    model.convs[0][0].register_forward_hook(get_activation(\"conv1\"))\n",
    "    filter_size = filter_sizes[0]\n",
    "elif retrieve_layer == \"conv2\":\n",
    "    model.convs[1][0].register_forward_hook(get_activation(\"conv2\"))\n",
    "    filter_size = filter_sizes[1] # + filter_sizes[0] // 2\n",
    "elif retrieve_layer == \"conv3\":\n",
    "    model.convs[2][0].register_forward_hook(get_activation(\"conv3\"))\n",
    "    filter_size = filter_sizes[2] # + filter_sizes[1] // 2\n",
    "    # filter_size = 13\n",
    "\n",
    "# Forward the test set\n",
    "with torch.no_grad():\n",
    "    for batch in loader:\n",
    "        out, loss = model(*batch)\n",
    "\n",
    "first_layer_output = activation[retrieve_layer].data.numpy()\n",
    "\n",
    "# Find maximal activation\n",
    "max_activation_val = first_layer_output.max()\n",
    "\n",
    "\n",
    "# Input data\n",
    "X = batch[0].cpu().numpy()\n",
    "y = batch[1].cpu().numpy()\n",
    "kernel_scores = np.array([\n",
    "    pearsonr(np.max(first_layer_output[:, i, :], axis=1), y)[0]\n",
    "     for i in range(first_layer_output.shape[1])\n",
    "])\n",
    "\n",
    "# Traverse every kernel\n",
    "kernel_data_dict2 = []\n",
    "df = pd.read_csv(\"../data/alphavirus/INFL_RandomMutants_Final.csv\")\n",
    "bg_seqs = df[\"seq\"].tolist()\n",
    "\n",
    "def generate_positional_counts_mat(seqs, centers, filter_size):\n",
    "    seqs = bg_seqs\n",
    "    counts_df_group = []\n",
    "    center_dict = dict(Counter(centers))\n",
    "    for i in range(filter_size):\n",
    "        nt_counts_all = {'A': 0, 'U': 0, 'C': 0, 'G': 0}\n",
    "        for center in center_dict.keys():\n",
    "            nt_counts = dict(Counter([s[center - filter_size // 2 + i] for s in seqs]))\n",
    "            for nt in nt_counts_all.keys():\n",
    "                nt_counts_all[nt] += nt_counts[nt] * center_dict[center]\n",
    "        counts_df_group.append(pd.DataFrame.from_records([nt_counts_all]))\n",
    "    counts_df = pd.concat(counts_df_group).reset_index(drop=True)\n",
    "    return counts_df\n",
    "\n",
    "for i in tqdm(range(first_layer_output.shape[1])):\n",
    "    kernel_output = first_layer_output[:, i, :]\n",
    "    max_val_locations = np.array(\n",
    "        [np.argmax(kernel_output[j, :]) for j in range(kernel_output.shape[0])]\n",
    "    )\n",
    "    max_activation_vals = np.max(kernel_output, axis=1)\n",
    "    # print(max_activation_vals)\n",
    "\n",
    "    # Only focus on the sequences which result in activation of 0.7 * maximum\n",
    "    indices = np.where(max_activation_vals > 0.2 * max_activation_val)[0]\n",
    "\n",
    "    # Convert embeddings back to sequences\n",
    "    seqs = [''.join([test_data.corpus[i] for i in X[idx, :]]) for idx in range(X.shape[0])]\n",
    "    motifs = []\n",
    "    centers = []\n",
    "    for j, center in enumerate(max_val_locations[indices]):\n",
    "        if center >= filter_size // 2 and center < len(seqs[0]) - filter_size // 2:\n",
    "            motif = seqs[j][center - filter_size // 2 : center + filter_size // 2 + 1]\n",
    "            if len(motif) == filter_size:  # Ensure the motif length is correct\n",
    "                motifs.append(motif)\n",
    "                centers.append(center)\n",
    "\n",
    "\n",
    "    if len(motifs) < 50:\n",
    "        counts_df = pd.DataFrame(0, index=range(filter_size), columns=[\"A\", \"U\", \"C\", \"G\"])\n",
    "        kernel_data_dict2.append((counts_df, i, 0, 0, 0)) \n",
    "    else:\n",
    "        bg_counts_df = generate_positional_counts_mat(bg_seqs, centers, filter_size)\n",
    "        # bg_counts_df = bg_counts_df[['A', 'C', 'G', 'T']]\n",
    "        \n",
    "        # print(centers)\n",
    "        counts_df = seqlogo_from_msa(motifs, bg_counts_mat=bg_counts_df) \n",
    "        kernel_data_dict2.append((counts_df, i, kernel_scores[i], mode(centers)[0], len(motifs)))\n",
    "        # kernel_data_dict.append((bg_counts_df, i, kernel_scores[i], mode(centers)[0], len(motifs)))\n",
    "        \n",
    "        # print(mode(centers)[0])\n",
    "\n",
    "# sort motif on location \n",
    "def takeSecond(elem):\n",
    "    return elem[1]\n",
    "kernel_data_dict2.sort(key=takeSecond)\n",
    "print(kernel_data_dict2)\n",
    "print(len(kernel_data_dict2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_data = []\n",
    "\n",
    "for counts_df, kernel_id, score, mode_center, motif_count in kernel_data_dict2:\n",
    "    for position in range(len(counts_df)):\n",
    "        row = counts_df.iloc[position]\n",
    "        flat_data.append({\n",
    "            \"kernel_id\": kernel_id,\n",
    "            \"position\": position,\n",
    "            \"score\": score,\n",
    "            \"mode_center\": mode_center,\n",
    "            \"motif_count\": motif_count,\n",
    "            \"A\": row[\"A\"],\n",
    "            \"U\": row[\"U\"],\n",
    "            \"C\": row[\"C\"],\n",
    "            \"G\": row[\"G\"]\n",
    "        })\n",
    "\n",
    "\n",
    "kernel_df = pd.DataFrame(flat_data)\n",
    "kernel_df.to_csv(\"./results/kernel_motifs_12.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PHATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import phate\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm  \n",
    "\n",
    "kernel_data_dict= [item for item in kernel_data_dict if item[3] != 0]\n",
    "kernel_data_dict2= [item for item in kernel_data_dict2 if item[3] != 0]\n",
    "\n",
    "features1 = [np.append(item[0].values.flatten(), item[3]*0.4) for item in kernel_data_dict]\n",
    "features2 = [np.append(item[0].values.flatten(), item[3]*0.4) for item in kernel_data_dict2]\n",
    "features = np.concatenate([features1, features2], axis=0)\n",
    "\n",
    "\n",
    "phate_operator = phate.PHATE(knn=3, decay=10, t=5)\n",
    "phate_embedding = phate_operator.fit_transform(features)\n",
    "\n",
    "scores1 = [item[2] for item in kernel_data_dict]\n",
    "scores2 = [item[2] for item in kernel_data_dict2]\n",
    "scores = scores1 + scores2  \n",
    "plt.figure(figsize=(12, 10))\n",
    "norm1 = plt.Normalize(vmin=min(scores1), vmax=max(scores1))  \n",
    "norm2 = plt.Normalize(vmin=min(scores2), vmax=max(scores2))  \n",
    "\n",
    "cmap1 = cm.Blues \n",
    "cmap2 = cm.Reds  \n",
    "\n",
    "scatter1 = plt.scatter(phate_embedding[:len(scores1), 0], phate_embedding[:len(scores1), 1], \n",
    "                       c=scores1, cmap=cmap1, norm=norm1, s=50)\n",
    "\n",
    "scatter2 = plt.scatter(phate_embedding[len(scores1):, 0], phate_embedding[len(scores1):, 1], \n",
    "                       c=scores2, cmap=cmap2, norm=norm2,  s=50)\n",
    "\n",
    "plt.xlabel(\"PHATE 1\")\n",
    "plt.ylabel(\"PHATE 2\")\n",
    "'''\n",
    "for i in range(len(kernel_data_dict)):\n",
    "    plt.text(phate_embedding[i, 0], phate_embedding[i, 1], \n",
    "             f\" {kernel_data_dict[i][1]}\", fontsize=10, color='black')\n",
    "'''\n",
    "\n",
    "\"\"\"for i in range(len(kernel_data_dict)):\n",
    "    plt.text(phate_embedding[i, 0], phate_embedding[i, 1], \n",
    "             f\" {kernel_data_dict[i][1]}\", fontsize=10, color='black')\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"for i in range(len(kernel_data_dict)):\n",
    "    if norm1(scores1[i]) >0.5:\n",
    "        plt.text(\n",
    "            phate_embedding[i, 0], \n",
    "            phate_embedding[i, 1], \n",
    "            f\" {kernel_data_dict[i][1]}\", \n",
    "            fontsize=10, color='blue'\n",
    "        )\n",
    "\n",
    "for i in range(len(kernel_data_dict2)):\n",
    "    if norm2(scores2[i]) >0.5:\n",
    "        plt.text(\n",
    "            phate_embedding[len(scores1) + i, 0], \n",
    "            phate_embedding[len(scores1) + i, 1], \n",
    "            f\" {kernel_data_dict2[i][1]}\", \n",
    "            fontsize=10, color='red'\n",
    "        )\"\"\"\n",
    "legend1_color = cmap1(norm1(max(scores1) - 0.2)) \n",
    "legend2_color = cmap2(norm2(max(scores2) - 0.2))  \n",
    "\n",
    "\n",
    "plt.scatter([], [], color=legend1_color, label='WT', s=50)  \n",
    "plt.scatter([], [], color=legend2_color, label='mut 1', s=50)  \n",
    "plt.legend(fontsize=15)\n",
    "plt.rcParams['svg.fonttype'] = 'none'\n",
    "plt.savefig('./results/phate_12.svg', format='svg', bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Patch\n",
    "\n",
    "n_clusters = 20\n",
    "kmeans = KMeans(\n",
    "    n_clusters=n_clusters,\n",
    "    init='k-means++',    \n",
    "    n_init=50,           # 随机初始化，防止局部最优\n",
    "    max_iter=2000,        \n",
    "    tol=1e-9,            # 收敛容忍度\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "cluster_labels = kmeans.fit_predict(phate_embedding)\n",
    "score = silhouette_score(phate_embedding, cluster_labels)\n",
    "print(f\"KMeans Silhouette Score: {score:.4f}\")\n",
    "\n",
    "cmap = cm.get_cmap('tab20', n_clusters)\n",
    "\n",
    "# plot\n",
    "plt.figure(figsize=(12, 10))\n",
    "scatter = plt.scatter(phate_embedding[:, 0], phate_embedding[:, 1],\n",
    "                      c=cluster_labels, cmap=cmap, s=50, alpha=0.8)\n",
    "plt.xlabel(\"PHATE 1\")\n",
    "plt.ylabel(\"PHATE 2\")\n",
    "\n",
    "legend_elements = [\n",
    "    Patch(facecolor=cmap(i), label=f\"Cluster {i}\") for i in range(n_clusters)\n",
    "]\n",
    "plt.legend(handles=legend_elements, title=\"Cluster ID\", fontsize=12,\n",
    "           bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.grid(True, linestyle='--', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.rcParams['svg.fonttype'] = 'none'\n",
    "plt.savefig('./results/phate_12_cluster.svg', format='svg', bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import numpy as np\n",
    "kernel_data_dict= [item for item in kernel_data_dict if item[3] != 0]\n",
    "centers = [item[3] + 1 for item in kernel_data_dict]\n",
    "scores = []\n",
    "labels = []\n",
    "for item in kernel_data_dict:\n",
    "    labels.append(item[1])\n",
    "    scores.append(item[2])\n",
    "norm = plt.Normalize(vmin=min(scores), vmax=max(scores))\n",
    "plt.figure(figsize=(12, 8))\n",
    "scatter = plt.scatter(\n",
    "    centers, scores, \n",
    "    c=scores, cmap=cm.Blues, norm=norm, s=50, label='Transcription'\n",
    ")\n",
    "\"\"\"for x, y, label in zip(centers2, scores2, labels):\n",
    "    plt.text(x, y, f\"{label}\", fontsize=10, ha='center', va='bottom')\"\"\"\n",
    "\n",
    "all_centers = sorted(set(centers)) \n",
    "plt.xticks(all_centers, fontsize=8) \n",
    "cbar = plt.colorbar(scatter, cmap=cm.coolwarm, norm=norm)\n",
    "cbar.set_label('filter importance')\n",
    "\n",
    "plt.xlabel(\"Center\", fontsize=14)\n",
    "plt.ylabel(\"importance\", fontsize=14)\n",
    "plt.rcParams['svg.fonttype']= 'none'\n",
    "plt.savefig('./results/WT_importance.svg', format='svg', bbox_inches='tight')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Draw all motifs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def takeSecond(elem):\n",
    "    return elem[1]\n",
    "kernel_data_dict.sort(key=takeSecond)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_kernel = len(kernel_data_dict)\n",
    "row = floor(sqrt(n_kernel))\n",
    "col = ceil(n_kernel / row)\n",
    "\n",
    "fig = plt.figure(figsize=(col * 4, row * 4))\n",
    "gs = plt.GridSpec(row * 2, col, figure=fig, height_ratios=[10, 1] * row)\n",
    "\n",
    "for i, (counts_df, kernel_idx, kernel_score, kernel_loc, n_motifs) in enumerate(kernel_data_dict[:n_kernel]):\n",
    "    if counts_df.empty or counts_df.shape[0] == 0:\n",
    "        print(f\"Skipping kernel {kernel_idx} due to empty counts_df.\")\n",
    "        continue\n",
    "    \n",
    "    ax = fig.add_subplot(gs[i // col * 2, i % col])\n",
    "    lm.Logo(counts_df, ax=ax, color_scheme=\"classic\")\n",
    "    #ax.set_ylim([0, 0.05])\n",
    "    max_height = counts_df.sum(axis=1).max()  \n",
    "    text_y = max_height + 0.09 * max_height\n",
    "    #text_y = max_height\n",
    "    #ax.set_xticks(list(range(filter_size)))\n",
    "    '''ax.text(\n",
    "        filter_size // 2, 0.97, \n",
    "        f\"Kernel {kernel_idx}: $r^2\\;=\\;{kernel_score:.3f}$\\ncenter={kernel_loc}, n={n_motifs}\",\n",
    "        ha='center', va='top', fontsize=15\n",
    "    )'''\n",
    "    ax.text(\n",
    "    filter_size // 2+0.02, text_y, \n",
    "    f\"Kernel {kernel_idx}: center={kernel_loc+1}\",ha='center', va='top', fontsize=17\n",
    "    )\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.set_yticks([])\n",
    "    ax.set_xticks([])  \n",
    "    ax.set_xticklabels([]) \n",
    "    #ax2 = fig.add_subplot(gs[i // col * 2 + 1, i % col])\n",
    "    '''kernel_score_dist = np.array([\n",
    "        pearsonr(first_layer_output[:, i, loc], y)[0]\n",
    "        for loc in range(first_layer_output.shape[2])\n",
    "    ])'''\n",
    "    #sns.heatmap(kernel_score_dist.reshape((1, -1)), ax=ax2, cmap='seismic', vmin=-1, vmax=1, cbar=False)\n",
    "    #ax2.axis('off')\n",
    "plt.rcParams['svg.fonttype']= 'none'\n",
    "plt.savefig('./results/WT_kernels.svg', format='svg', bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saliency map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def predict_seq_score(seq):\n",
    "    X = torch.tensor([test_data.tok2idx[s] for s in seq]).unsqueeze(0)\n",
    "    original_score, _ = model(X,None)\n",
    "    original_score = original_score.detach().numpy()\n",
    "    return original_score[0]\n",
    "\n",
    "# Define the original sequence and compute its score\n",
    "original_seq = \"UCGUUUUCGUCCCACUGUUUUUGUAU\"\n",
    "original_score = predict_seq_score(original_seq)\n",
    "saliency_arr = np.zeros((4, len(original_seq)))\n",
    "original=\"UCGUUUUCGUCCCACUGUUUUUGUAU\"\n",
    "\n",
    "# Compute saliency scores for each nucleotide position\n",
    "for i in range(len(original_seq)):\n",
    "    for j, nt in enumerate(['A', 'U', 'C', 'G']):\n",
    "        saliency_arr[j, i] = predict_seq_score(original_seq[:i] + nt + original_seq[i+1:])\n",
    "saliency_arr -= original_score\n",
    "\n",
    "# Define the range for color mapping\n",
    "vmin = saliency_arr.min()  # Minimum value for the color scale\n",
    "vmax = saliency_arr.max()  # Maximum value for the color scale\n",
    "\n",
    "# Plotting\n",
    "_, ax = plt.subplots(1, 1, figsize=(10, 2))\n",
    "sns.heatmap(saliency_arr, cmap='coolwarm', center=0, ax=ax, vmin=vmin, vmax=vmax)\n",
    "\n",
    "# Set the ticks and labels correctly\n",
    "num_positions = len(original_seq)\n",
    "ax.set_yticks(np.arange(4) + 0.5)\n",
    "ax.set_yticklabels(['A', 'U', 'C', 'G'])\n",
    "ax.set_xticks(np.arange(num_positions) + 0.5)\n",
    "ax.set_xticklabels(np.arange(1, num_positions + 1), fontsize=10)\n",
    "\n",
    "# Add black points for the original sequence\n",
    "original_seq_idx = np.array(['AUCG'.find(nt) for nt in original])\n",
    "ax.scatter(np.arange(len(original_seq)) + 0.5, original_seq_idx + 0.5, c=\"k\", zorder=5)\n",
    "plt.rcParams['svg.fonttype']= 'none'\n",
    "plt.savefig('./results//saliency.svg', format='svg', bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "saliency_df = pd.DataFrame(\n",
    "    saliency_arr,\n",
    "    index=['A', 'U', 'C', 'G'],\n",
    "    columns=[f\"pos_{i+1}\" for i in range(len(original_seq))]\n",
    ")\n",
    "saliency_df.to_csv('./results//saliency.csv')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
